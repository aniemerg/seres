# Agent Prompt Cache Analysis

## Current Cache Hit Rate: 54.9% âš ï¸

This is LOW. We should be getting 80-90%+ cache hit rate.

## Problem Identified

**The agent is NOT being reset between queue items**, causing conversation history to accumulate and break caching.

## Current Prompt Structure

### Agent Initialization (Once per worker launch)
```python
# Line 432 in worker.py
agent = build_agent(model)  # Built ONCE

while True:  # Process multiple items with SAME agent
    # Item 1, 2, 3, ... all use same agent instance
```

### Prompt Construction (Per Agent Build)

**Static Content (Should be 100% cached):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SYSTEM MESSAGE (agent.instructions)        â”‚
â”‚                                             â”‚
â”‚ 1. cached_context.md (~7,927 tokens)       â”‚
â”‚    - Agent reference memo                   â”‚
â”‚    - KB schema guide                        â”‚
â”‚    - Complex examples (processes, recipes)  â”‚
â”‚    - Papers directory                       â”‚
â”‚    - Queue workflow docs                    â”‚
â”‚                                             â”‚
â”‚ 2. AGENT_INSTRUCTIONS (~300 tokens)         â”‚
â”‚    - Your goal                              â”‚
â”‚    - Process steps                          â”‚
â”‚    - Output format                          â”‚
â”‚    - Important notes                        â”‚
â”‚                                             â”‚
â”‚ Total: ~8,227 tokens (STATIC) âœ“            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Variable Content (Changes per item/iteration)

**Iteration 1:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER MESSAGE 1                              â”‚
â”‚ "Please process the queue item..."         â”‚
â”‚ + Queue item JSON (lease_result)           â”‚
â”‚   - item_id (VARIABLE)                      â”‚
â”‚   - gap_type (VARIABLE)                     â”‚
â”‚   - context (VARIABLE)                      â”‚
â”‚ Total: ~100-500 tokens (VARIABLE) âœ—        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AGENT RESPONSE 1                            â”‚
â”‚ - Text output (VARIABLE)                    â”‚
â”‚ - Tool calls (VARIABLE)                     â”‚
â”‚   - rg_search(...)                          â”‚
â”‚   - read_file(...)                          â”‚
â”‚   - write_file(...)                         â”‚
â”‚   - run_indexer()                           â”‚
â”‚ Total: ~1,000-5,000 tokens (VARIABLE) âœ—    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TOOL RESULTS 1                              â”‚
â”‚ - Search results (VARIABLE)                 â”‚
â”‚ - File contents (VARIABLE)                  â”‚
â”‚ - Indexer output (VARIABLE)                 â”‚
â”‚ Total: ~500-3,000 tokens (VARIABLE) âœ—      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Iteration 2 (if needed):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER MESSAGE 2                              â”‚
â”‚ "The indexer failed..." or                  â”‚
â”‚ "The gap still exists..."                   â”‚
â”‚ + Error details (VARIABLE)                  â”‚
â”‚ Total: ~100-1,000 tokens (VARIABLE) âœ—      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AGENT RESPONSE 2 + TOOL RESULTS 2           â”‚
â”‚ More variable content...                    â”‚
â”‚ Total: ~1,000-8,000 tokens (VARIABLE) âœ—    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Iteration 3 (if needed):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER MESSAGE 3 + AGENT RESPONSE 3           â”‚
â”‚ Even more variable content...               â”‚
â”‚ Total: ~1,000-8,000 tokens (VARIABLE) âœ—    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## The Critical Problem: Agent Reuse Across Items

```python
# CURRENT CODE (BAD):
agent = build_agent(model)  # Line 432

while True:
    # Item 1 processing
    lease_item_1()
    run_agent_streamed(agent, input_1)  # Conversation: [system, user1, agent1, tools1]

    # Item 2 processing - SAME AGENT!
    lease_item_2()
    run_agent_streamed(agent, input_2)  # Conversation: [system, user1, agent1, tools1, user2, agent2, tools2]

    # Item 3 processing - SAME AGENT!
    lease_item_3()
    run_agent_streamed(agent, input_3)  # Conversation: [system, user1, agent1, tools1, user2, agent2, tools2, user3, agent3, tools3]
```

**Result:**
- Static content (8,227 tokens): Cached once âœ“
- Item 1 variable content (~5,000 tokens): Not cached âœ—
- Item 2 variable content (~5,000 tokens): Not cached âœ—
- Item 3 variable content (~5,000 tokens): Not cached âœ—
- ...
- Item 50 variable content (~5,000 tokens): Not cached âœ—

**Effective cache rate:**
```
Cached: 8,227 tokens
Total: 8,227 + (50 items Ã— 5,000 tokens) = 258,227 tokens
Cache rate: 8,227 / 258,227 = 3.2% ğŸ˜±
```

But we're seeing 54.9%, which means the Agent SDK is doing SOME caching of the growing conversation, but not optimally.

## Why Cache Rate is Low

1. **Agent persists across items** - Conversation history accumulates
2. **Variable content accumulates** - Each item adds ~5K+ variable tokens
3. **Cache breaking** - Variable content in middle of conversation breaks prefix caching
4. **Long conversations** - Multi-iteration items create very long conversations

## Solution: Rebuild Agent for Each Item

**What we should do:**

```python
# PROPOSED CODE (GOOD):
while True:
    # Lease item
    lease_result = execute_queue_lease(agent_name)

    # BUILD FRESH AGENT FOR THIS ITEM
    agent = build_agent(model)  # Fresh conversation!

    # Process item
    run_agent_streamed(agent, input_with_lease)

    # After completion/release, agent is discarded
    # Next iteration gets a fresh agent
```

**Expected cache rate:**

For each item:
```
Static content: 8,227 tokens (100% cached after first item)
Variable content: ~5,000 tokens (not cached, but only per-item)

Cache rate: 8,227 / (8,227 + 5,000) = 62.2%
```

But with multi-iteration items (3 iterations max):
```
Static content: 8,227 tokens (cached)
Iteration 1 variable: ~5,000 tokens
Iteration 2 variable: ~5,000 tokens
Iteration 3 variable: ~5,000 tokens
Total: 8,227 + 15,000 = 23,227 tokens

Cache rate: 8,227 / 23,227 = 35.4% per item
```

But across many items, first-pass success (no iterations) should dominate:
```
Successful items (80%): 8,227 / 13,227 = 62.2% cache rate
Failed items (20%): 8,227 / 23,227 = 35.4% cache rate

Weighted average: 0.8 Ã— 62.2% + 0.2 Ã— 35.4% = 56.8%
```

Still not great! We need to go further...

## Further Optimization: Move Variable Content to Tool Response

Instead of putting queue item in user message, inject it as a "pseudo tool result":

```python
# BETTER APPROACH:
agent = build_agent(model)

# Inject lease as if it was a tool response
initial_messages = [
    {
        "role": "user",
        "content": "Please process the next queue item."
    },
    {
        "role": "assistant",
        "content": "",
        "tool_calls": [{
            "id": "lease_1",
            "type": "function",
            "function": {
                "name": "queue_lease",
                "arguments": "{}"
            }
        }]
    },
    {
        "role": "tool",
        "tool_call_id": "lease_1",
        "content": json.dumps(lease_result)  # Variable content here
    }
]
```

This puts variable content AFTER the cacheable system message, allowing better caching.

## Theoretical Maximum Cache Rate

If we optimize perfectly:
```
Static system message: 8,227 tokens (always cached)
User prompt: ~50 tokens (always same: "Please process the next queue item")
Tool result: ~500 tokens (variable lease data)

Per item: 8,227 / (8,227 + 50 + 500) = 93.7% cache rate ğŸ¯
```

## Recommendations

### Priority 1: Rebuild Agent Per Item (CRITICAL)
```python
# Change line 432 from:
agent = build_agent(model)
while True:
    # ... process item with same agent

# To:
while True:
    agent = build_agent(model)  # Fresh agent each item!
    # ... process item
```

**Expected improvement:** 3.2% â†’ 56.8% cache rate

### Priority 2: Separate Static from Variable in User Message
Instead of:
```python
input_with_lease = f"{user_input}\n\n{json.dumps(lease_result)}"
```

Use:
```python
user_input = "Please process the next queue item."
# Pass lease_result as a tool response (requires Agent SDK changes)
```

**Expected improvement:** 56.8% â†’ 85%+ cache rate

### Priority 3: Minimize Iteration Feedback Size
Current:
```python
user_input = f"The indexer failed with errors. Please fix them:\n{chr(10).join(errors[:10])}"
```

Better:
```python
user_input = "The indexer reported errors. Please review and fix them."
# Store errors in a file that agent can read_file()
```

**Expected improvement:** 85% â†’ 90%+ cache rate

## Summary

| Optimization | Cache Rate | Effort |
|-------------|------------|--------|
| Current (broken) | 54.9% | - |
| Priority 1: Rebuild agent | 56.8% | 5 minutes |
| Priority 2: Tool response pattern | 85% | 1 hour |
| Priority 3: Minimize feedback | 90%+ | 30 minutes |

**Immediate action: Move `agent = build_agent(model)` inside the while loop.**
